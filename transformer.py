import torch
torch.cuda.is_available() # GPU í• ë‹¹ì—¬ë¶€ í™•ì¸. GPUê°€ ì •ìƒì ìœ¼ë¡œ í• ë‹¹ë˜ë©´ Trueê°€ ì¶œë ¥.

"""### 1ï¸âƒ£ Dataset & Tokenizing
- torch.utils.data.Dataset class ë¥¼ ìƒì†ë°›ì•„ news_dataset classë¥¼ ì •ì˜í•´ì¤ë‹ˆë‹¤.
- ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” load_data ì™€ tokenizingì„ ì§„í–‰í•˜ëŠ” <br> construct_tokenized_dataset ë¥¼ ì •ì˜í•´ì¤ë‹ˆë‹¤.
-  ìœ„ì—ì„œ ì •ì˜ëœ í•¨ìˆ˜ë“¤ì„ í™œìš©í•˜ì—¬, ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì™€ tokenizing í•œí›„ì—, <br> torch dataset classë¡œ ë³€í™˜í•´ì¤ë‹ˆë‹¤.
- ì´í›„ train data /validation data ëŠ” 7.5 : 2.5ë¡œ ë‚˜ëˆ ì¤ë‹ˆë‹¤.

####1-1. <b> news_dataset class </b> ì •ì˜
"""

import os
import pandas as pd
import torch

class news_dataset(torch.utils.data.Dataset):
    """dataframeì„ torch dataset classë¡œ ë³€í™˜"""
    def __init__(self, news_dataset, labels):
        self.dataset = news_dataset
        self.labels = labels

    def __getitem__(self,idx):
        item = {
            key: val[idx].clone().detach() for key, val in self.dataset.items() # keyì™€ valueë¥¼ ê°ê° ì¸ë±ìŠ¤ì— ë§ê²Œ ë„£ì–´ì¤Œ
        }
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

"""####1-2. <b>load_data , construct_tokenized_dataset</b> í•¨ìˆ˜ ì •ì˜
- "ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³ " "í† í¬ë‚˜ì´ì§• í•´ì£¼ëŠ”" í•¨ìˆ˜ë“¤
"""

def load_data(dataset_dir):
    """csv fileì„ dataframeìœ¼ë¡œ load"""
    dataset = pd.read_csv(dataset_dir)[:500] # ì¼ë‹¨ 500ê°œë§Œ ë¶ˆëŸ¬ì˜´
    print("dataframe ì˜ í˜•íƒœ")
    print(dataset.head())
    return dataset

def construct_tokenized_dataset(dataset,tokenizer, max_length):
    """[ë‰´ìŠ¤ì œëª© + [SEP] + ë‰´ìŠ¤ë³¸ë¬¸]í˜•íƒœë¡œ í† í¬ë‚˜ì´ì§•""" # ì´ë ‡ê²Œ ì—®ì–´ì¤„ ìƒê°!
    concat_entity = []
    for title, body in zip(dataset["newsTitle"],dataset["newsContent"]):
        total = str(title) + "[SEP]" + str(body) # ì—¬ê¸°ì„œ ì—®ì–´ì¤Œ # ë¬¸ì¥ êµ¬ë¶„ì„ ìœ„í•œ [SEP] íŠ¹ìˆ˜ í† í°
        concat_entity.append(total) # ê° ë¬¸ì¥ì„ concat_entity ë¦¬ìŠ¤íŠ¸ì— ë¶™ì—¬ì£¼ê¸°
    print("tokenizer ì— ë“¤ì–´ê°€ëŠ” ë°ì´í„° í˜•íƒœ")
    print(concat_entity[:5])
    tokenized_senetences = tokenizer( # ë¶™ì—¬ë†“ì€ ê±¸ ê°€ì ¸ì™€ì„œ í† í¬ë‚˜ì´ì§• í•´ì£¼ê¸°
        concat_entity,
        return_tensors = "pt",
        padding = True,
        truncation = True,
        max_length = max_length,
        add_special_tokens = True,
        # return_token_type_ids=False, # BERT ì´í›„ ëª¨ë¸(RoBERTa ë“±) ì‚¬ìš©í• ë•Œ False
    )
    print("tokenizing ëœ ë°ì´í„° í˜•íƒœ")
    print(tokenized_senetences[:5])
    return tokenized_senetences

"""####1-3. prepare_dataset í•¨ìˆ˜ ì •ì˜
- ì•ì„œ ì •ì˜í•œ í•¨ìˆ˜ë“¤ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ì…‹ ì¤€ë¹„í•˜ëŠ” í•¨ìˆ˜

ğŸŒŸ ì „ì²˜ë¦¬ê³¼ì • ğŸŒŸ<blockquote>
1. train.csv / test.csv íŒŒì¼ì„ pd.dataframe ë¡œ ë‹¤ìš´ë¡œë“œ í•´ì¤€ë‹¤. <br>
2. train/validation setì„ ë‚˜ëˆ ì¤€ë‹¤. (7.5:2.5) <br>
3. label ê°’ì„ ë”°ë¡œ ì €ì¥í•´ì¤€ë‹¤. <br>
4. ì œëª©ê³¼ ë³¸ë¬¸ ë°ì´í„°ë§Œ ì •ì œí•œí›„ì— tokenizing í•´ì¤€ë‹¤. <br>
5. tokenizing ëœ ë°ì´í„°ë¥¼ news_dataset classë¡œ ë°˜í™˜í•´ì¤€ë‹¤. <br>
"""

def prepare_dataset(dataset_dir, tokenizer,max_len):
    """í•™ìŠµ(train)ê³¼ í‰ê°€(test)ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ì„ ì¤€ë¹„"""
    # load_data
    train_dataset = load_data(os.path.join(dataset_dir, "train.csv"))
    test_dataset = load_data(os.path.join(dataset_dir, "test.csv"))

    # split train / validation = 7.5 : 2.5
    train_dataset, val_dataset = train_test_split(train_dataset,test_size=0.25,random_state=42,stratify=train_dataset['label']) # stratify: ë‚˜ëˆŒ ë•Œ, í´ë˜ìŠ¤ ë¹„ìœ¨ë„ ë™ì¼í•˜ê²Œ ë‚˜ëˆ ì¤Œ!

    # split label
    train_label = train_dataset['label'].values
    val_label = val_dataset['label'].values
    test_label = test_dataset['label'].values

    # tokenizing dataset
    tokenized_train = construct_tokenized_dataset(train_dataset, tokenizer, max_len)
    tokenized_val = construct_tokenized_dataset(val_dataset, tokenizer, max_len)
    tokenized_test = construct_tokenized_dataset(test_dataset, tokenizer, max_len)
    print("--- tokenizing Done ---") # ì´ë ‡ê²Œ ë¡œê¹…ì„ í•´ì£¼ë©´ ì¢‹ìŒ!

    # make dataset for pytorch.
    news_train_dataset = news_dataset(tokenized_train, train_label)
    news_val_dataset = news_dataset(tokenized_val, val_label)
    news_test_dataset = news_dataset(tokenized_test, test_label)
    print("--- dataset class Done ---")

    return news_train_dataset , news_val_dataset, news_test_dataset , test_dataset

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')

df = load_data("./data/train.csv")

tokenized_data = construct_tokenized_dataset(df,tokenizer,120)

for key ,value in tokenized_data.items():
    print(key)

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base')


"""### 2ï¸âƒ£ Model & Trainer
- huggingface ì—ì„œ ì‚¬ì „í•™ìŠµëœ(pre-trained) ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
- huggingface ì˜ Trainer ëª¨ë“ˆì„ ì •ì˜í•˜ê³  í•™ìŠµì— ì‚¬ìš©ë  Arguments ë“¤ì„ ì§€ì •í•´ì¤ë‹ˆë‹¤.
"""

import os
import random
import numpy as np
import pytorch_lightning as pl
import torch
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import train_test_split

from transformers import (
    AutoTokenizer,
    AutoConfig,
    AutoModelForSequenceClassification,
)
from transformers import Trainer, TrainingArguments
from transformers import EarlyStoppingCallback
from transformers.optimization import get_cosine_with_hard_restarts_schedule_with_warmup

"""####2-1. compute_metrics í•¨ìˆ˜ ì •ì˜
- í•™ìŠµ ì¤‘ validation í• ë•Œ ì‚¬ìš©ë  í‰ê°€ì§€í‘œ ì •ì˜í•˜ëŠ” í•¨ìˆ˜
- í•´ë‹¹ ì‹¤ìŠµì—ì„œëŠ” Accuracyì™€ F1 Scoreë¥¼ Metricìœ¼ë¡œ ì‚¬ìš©
"""

def compute_metrics(pred):
    """validationì„ ìœ„í•œ metrics function"""
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)

    # calculate accuracy using sklearn's function
    acc = accuracy_score(labels, preds)

    # calculate f1 score using sklearn's function
    f1 = f1_score(labels, preds, average='micro')

    return {
        "accuracy": acc,
        "f1": f1,
    }

"""####2-2.load_tokenizer_and_model_for_train í•¨ìˆ˜ ì •ì˜
- í•™ìŠµì— ì‚¬ìš©ë  í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
"""

def load_tokenizer_and_model_for_train():
    """í•™ìŠµ(train)ì„ ìœ„í•œ ì‚¬ì „í•™ìŠµ(pretrained) í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì„ huggingfaceì—ì„œ load"""
    # load model and tokenizer
    MODEL_NAME = args.model_name
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    # setting model hyperparameter # configë¥¼ ë”°ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ì´ìœ : labelì´ 2ê°€ ì•„ë‹ ìˆ˜ë„ ìˆì–´ì„œ;
    model_config = AutoConfig.from_pretrained(MODEL_NAME)
    model_config.num_labels = 2
    print(model_config)

    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME, config=model_config
    )
    print("--- Modeling Done ---")
    return tokenizer , model

"""####2-3.load_trainer_for_train í•¨ìˆ˜ ì •ì˜
- í•™ìŠµì— ì‚¬ìš©ë  Trainer ëª¨ë“ˆì„ ì •ì˜í•˜ê³  Argumentsë“¤ì„ ì§€ì •í•´ì¤€ë‹¤.
"""

def load_trainer_for_train(model,news_train_dataset,news_val_dataset):
    """í•™ìŠµ(train)ì„ ìœ„í•œ huggingface trainer ì„¤ì •"""
    training_args = TrainingArguments(
        output_dir=args.save_path + "results",  # output directory
        save_total_limit=args.save_limit,  # number of total save model.
        save_steps=args.save_step,  # model saving step.
        num_train_epochs=args.epochs,  # total number of training epochs
        learning_rate=args.lr,  # learning_rate
        per_device_train_batch_size=args.batch_size,  # batch size per device during training
        per_device_eval_batch_size=2,  # batch size for evaluation
        warmup_steps=args.warmup_steps,  # number of warmup steps for learning rate scheduler
        weight_decay=args.weight_decay,  # strength of weight decay
        logging_dir=args.save_path + "logs",  # directory for storing logs
        logging_steps=args.logging_step,  # log saving step.
        eval_strategy="steps",  # evaluation strategy to adopt during training # valì—ì„œì˜ eval
            # `no`: No evaluation during training.
            # `steps`: Evaluate every `eval_steps`.
            # `epoch`: Evaluate every end of epoch.
        eval_steps=args.eval_step,  # evaluation step.
        load_best_model_at_end=True,
    )

    ## Add callback & optimizer & scheduler
    MyCallback = EarlyStoppingCallback(
        early_stopping_patience=3, early_stopping_threshold=0.001
    )

    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.lr,
        betas=(0.9, 0.999),
        eps=1e-08,
        weight_decay=args.weight_decay,
        amsgrad=False,
    )
    print("--- Set training arguments Done ---")

    trainer = Trainer(
        model=model,  # the instantiated ğŸ¤— Transformers model to be trained
        args=training_args,  # training arguments, defined above
        train_dataset=news_train_dataset,  # training dataset
        eval_dataset=news_val_dataset,  # evaluation dataset
        compute_metrics=compute_metrics,  # define metrics function
        callbacks=[MyCallback],
        optimizers=(
            optimizer,
            get_cosine_with_hard_restarts_schedule_with_warmup(
                    optimizer,
                    num_warmup_steps=args.warmup_steps,
                    num_training_steps=len(news_train_dataset) * args.epochs,
            ),
        ),
    )
    print("--- Set Trainer Done ---")

    return trainer

"""####2-4.train í•¨ìˆ˜ ì •ì˜
- ì‹¤í—˜ì„¸íŒ… í›„ ì•ì„œ ì •ì˜í•œ í•¨ìˆ˜ë“¤ í™œìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•˜ëŠ” í•¨ìˆ˜

ğŸŒŸ í•™ìŠµë™ì‘ê³¼ì • ğŸŒŸ
<blockquote>
1. ì‹¤í—˜ì— ì˜í–¥ì„ ì£¼ëŠ” ëª¨ë“  seedë¥¼ ê³ ì •í•´ì¤€ë‹¤. <br>
2. ì‚¬ìš©í•  gpuë¥¼ deviceì— í• ë‹¹í•´ì¤€ë‹¤. <br>
3. tokenizerì™€ modelì„ ë¶ˆëŸ¬ì˜¨í›„, modelì„ deviceì— í• ë‹¹í•´ì¤€ë‹¤. <br>
4. í•™ìŠµì— ì‚¬ìš©ë  news_dataset ì„ ë¶ˆëŸ¬ì˜¨ë‹¤.<br>
5. í•™ìŠµì— ì‚¬ìš©ë  Trainer ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.<br>
6. í•™ìŠµì„ ì§„í–‰í•œí›„ì— best_modelì„ ì €ì¥í•´ì¤€ë‹¤. <br>
"""

def train():
    """ëª¨ë¸ì„ í•™ìŠµ(train)í•˜ê³  best modelì„ ì €ì¥"""
    # fix a seed
    pl.seed_everything(seed=42, workers=False)

    # set device
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print("device:", device)

    # set model and tokenizer
    tokenizer , model = load_tokenizer_and_model_for_train()
    model.to(device)

    # set data
    news_train_dataset, news_val_dataset, news_test_dataset, test_dataset = prepare_dataset(args.dataset_dir,tokenizer,args.max_len) # í‰ê°€í•  ë•Œ ì“°ë ¤ê³  test_dataset ë”°ë¡œ ë¹¼ë‘ 

    # set trainer
    trainer = load_trainer_for_train(model,news_train_dataset,news_val_dataset)

    # train model
    print("--- Start train ---")
    trainer.train()
    print("--- Finish train ---")
    model.save_pretrained("./best_model")

"""####2-5.arguments ì§€ì • ë° í•™ìŠµ ì§„í–‰
- RoBERTa ëª¨ë¸ ê°„ëµ ì„¤ëª…

<blockquote>
1. BERT ëª¨ë¸ì˜ ë³€í˜•ìœ¼ë¡œ, í•™ìŠµ ë°ì´í„°ì˜ ì–‘ì„ í¬ê²Œ ëŠ˜ë¦¬ê³  í•™ìŠµë¥ (learning rate)ì„ ì¡°ì •í•˜ë©°, <br>ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ ë‹¤ì–‘í™”í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤. <br>
2. BERTì˜ ì‚¬ì „í•™ìŠµ ë°©ë²• ì¤‘ í•˜ë‚˜ì¸ NSP(Next Sentence Prediction)ì„ ì œê±°í•˜ì˜€ë‹¤.
"""

class args (): # í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” íŒŒë¼ë¯¸í„°ë“¤ì„ ì—¬ê¸°ì— ëª¨ì•„ë†“ìŒ
  """í•™ìŠµ(train)ê³¼ ì¶”ë¡ (infer)ì— ì‚¬ìš©ë˜ëŠ” arguments ê´€ë¦¬í•˜ëŠ” class"""
  dataset_dir = "./data"
  model_type = "roberta" # ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ e.g) "bert" , "electra" Â·Â·Â·
  model_name = "klue/roberta-large" # ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ e.g) "klue/bert-base" , "monologg/koelectra-base-finetuned-nsmc" Â·Â·Â·
  save_path = "./"
  save_step = 200
  logging_step = 200
  eval_step = 100
  save_limit = 5
  seed = 42
  epochs = 1 # 10
  batch_size = 8 # ë©”ëª¨ë¦¬ ìƒí™©ì— ë§ê²Œ ì¡°ì ˆ e.g) 16 or 32
  max_len = 256
  lr = 3e-5
  weight_decay = 0.01
  warmup_steps = 300
  scheduler = "linear"
  model_dir = "./best_model" #ì¶”ë¡  ì‹œ, ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” ê²½ë¡œ ì„¤ì •

train()

"""### 3ï¸âƒ£ Inference & Evaluation
- í•™ìŠµì™„ë£Œëœ(fine-tuned) ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ì¶”ë¡ (infer)ì„ ì§„í–‰í•©ë‹ˆë‹¤.
- ì¶”ë¡ ëœ ì˜ˆì¸¡ê°’ë“¤ê³¼ ì •ë‹µê°’ì„ ë¹„êµí•˜ì—¬ í‰ê°€(evaluation)í•©ë‹ˆë‹¤.
"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.utils.data import DataLoader
import pandas as pd
import torch
import torch.nn.functional as F

import numpy as np
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score

"""####3-1.load_model_for_inference í•¨ìˆ˜ ì •ì˜
- í•™ìŠµëœ(fine-tuned) ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸(checkpoint)ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ <br>
<b>(ì´ë•Œ, í† í¬ë‚˜ì´ì €ëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ huggingface ì—ì„œ ë¶ˆëŸ¬ì˜¨ë‹¤. )</b>

"""

def load_model_for_inference():
    """ì¶”ë¡ (infer)ì— í•„ìš”í•œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € load """
    # load tokenizer
    Tokenizer_NAME = args.model_name
    tokenizer = AutoTokenizer.from_pretrained(Tokenizer_NAME)

    ## load my model
    model = AutoModelForSequenceClassification.from_pretrained(args.model_dir) # ì—¬ê¸°ì„  model nameë§ê³ , directory ë„£ì–´ì¤Œ. ì™œ? í•™ìŠµí•œ ê±¸ ë¶ˆëŸ¬ì™€ì•¼ ë˜ê¸° ë•Œë¬¸!

    return tokenizer, model

"""####3-2. inference í•¨ìˆ˜ ì •ì˜
- í•™ìŠµëœ(fine-tuned)ëª¨ë¸ì„ í†µí•´ í‰ê°€ ë°ì´í„°ì˜ ì˜ˆì¸¡ê°’ ì¶”ë¡ í•´ë‚´ëŠ” í•¨ìˆ˜

ğŸŒŸ ì¶”ë¡ (infer) ê³„ì‚°ê³¼ì • ğŸŒŸ

<blockquote>
1. model.eval , torh.no_grad ë¥¼ í†µí•´ ëª¨ë¸ì„ ì¶”ë¡  ëª¨ë“œë¡œ ë³€ê²½ <br>
2. ëª¨ë¸ì— ì…ë ¥ê°’ìœ¼ë¡œ input_ids ì™€ attention maskë¥¼ <b>gpuì— í• ë‹¹í•œ í›„</b> ì…ë ¥ìœ¼ë¡œ ì£¼ê³  ê²°ê³¼ê°’(outputs) ìƒì„± <br>
3. ê²°ê³¼ê°’(outputs) ì¤‘ logits ê°’ì„ cpuë¡œ í• ë‹¹í•œ í›„, argmax ë¥¼ í†µí•´ ì˜ˆì¸¡ ë ˆì´ë¸”(label) ìƒì„± <br>
4. ìƒì„±ëœ ë ˆì´ë¸”(label) ì„ concat í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜
"""

def inference(model, tokenized_sent, device):
    """í•™ìŠµëœ(trained) ëª¨ë¸ì„ í†µí•´ ê²°ê³¼ë¥¼ ì¶”ë¡ í•˜ëŠ” function"""
    dataloader = DataLoader(tokenized_sent, batch_size=args.batch_size, shuffle=False)
    model.eval()
    output_pred = []
    for i, data in enumerate(tqdm(dataloader)):
        with torch.no_grad():
            outputs = model(
                input_ids=data["input_ids"].to(device),
                attention_mask=data["attention_mask"].to(device),
            )
        logits = outputs[0]
        logits = logits.detach().cpu().numpy()
        result = np.argmax(logits, axis=-1)

        output_pred.append(result)
    return (np.concatenate(output_pred).tolist(),)

"""####3-3.infer_and_eval í•¨ìˆ˜ ì •ì˜
- í•™ìŠµëœ(fine-tuned) ëª¨ë¸ë¡œ ì¶”ë¡ (infer)í•œ í›„ ì˜ˆì¸¡í•œ ê²°ê³¼ë¥¼ í‰ê°€(evaluation)í•˜ëŠ” í•¨ìˆ˜

<blockquote>
1. ì‚¬ìš©í•  gpuë¥¼ deviceì— í• ë‹¹í•´ì¤€ë‹¤. <br>
2. tokenizerì™€ modelì„ ë¶ˆëŸ¬ì˜¨í›„, modelì„ deviceì— í• ë‹¹í•´ì¤€ë‹¤. <br>
3. ì¶”ë¡ ì— ì‚¬ìš©ë  news_dataset ì„ ë¶ˆëŸ¬ì˜¨ë‹¤.<br>
4. model ê³¼ news_datasetì„ ì…ë ¥ìœ¼ë¡œ ì£¼ê³  ì¶”ë¡ (infer)ì„ ì§„í–‰í•œë‹¤. <br>
5. test data ì˜ ë ˆì´ë¸”(label)ê³¼ ì˜ˆì¸¡ê°’(pred)ì„ ë¹„êµí•˜ì—¬ í‰ê°€ì§€í‘œë¥¼ ê³„ì‚°í•œë‹¤.<br>
6. ìµœì¢… ì˜ˆì¸¡ê°’ì„ csv í˜•íƒœë¡œ ì €ì¥í•´ì¤€ë‹¤. <br>
"""

def infer_and_eval():
    """í•™ìŠµëœ ëª¨ë¸ë¡œ ì¶”ë¡ (infer)í•œ í›„ì— ì˜ˆì¸¡í•œ ê²°ê³¼(pred)ë¥¼ í‰ê°€(eval)"""
    # set device
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    # set model & tokenizer
    tokenizer, model = load_model_for_inference()
    model.to(device)

    # set data
    news_train_dataset, news_val_dataset, news_test_dataset, test_dataset = prepare_dataset(args.dataset_dir,tokenizer,args.max_len)

    # predict answer
    pred_answer = inference(model, news_test_dataset, device)  # modelì—ì„œ class ì¶”ë¡ 
    print("--- Prediction done ---")

    # evaluate between label and prediction
    labels = test_dataset['label'].values
    pred = pred_answer[0]

    acc = accuracy_score(labels, pred)
    f1 = f1_score(labels, pred, average='macro')
    print(f" ----- accuracy:{acc * 100:.1f}% -----")
    print(f"----- f1_score(macro): {f1 * 100:.1f}% ------")

    # make csv file with predicted answer
    output = pd.DataFrame(
        {
            "title": test_dataset["newsTitle"],
            "cleanBody": test_dataset["newsContent"],
            "clickbaitClass": pred,
        }
    )

    # ìµœì¢…ì ìœ¼ë¡œ ì™„ì„±ëœ ì˜ˆì¸¡í•œ ë¼ë²¨ csv íŒŒì¼ í˜•íƒœë¡œ ì €ì¥.
    result_path = "./prediction/"
    if not os.path.exists(result_path):
        os.makedirs(result_path)
    output.to_csv(
        os.path.join(result_path,"result.csv"), index=False
    )
    print("--- Save result ---")
    return output # ë°‘ì—ì„œ ì°ì–´ë³´ë ¤ê³  ì¶”ê°€í•œ ê±°ì„(ì—†ì–´ë„ ë¨)

"""- ì¶”ë¡  ë° í‰ê°€ ì§„í–‰ í›„ ê²°ê³¼ê°’ 10ê°œê¹Œì§€ ì¶œë ¥"""

output_df = infer_and_eval()
output_df.head(10)